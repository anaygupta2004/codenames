/*************************************************************************************************

Welcome to Baml! To use this generated code, please run one of the following:

$ npm install @boundaryml/baml
$ yarn add @boundaryml/baml
$ pnpm add @boundaryml/baml

*************************************************************************************************/

// This file was generated by BAML: do not edit it. Instead, edit the BAML
// files and re-generate this code.
//
/* eslint-disable */
// tslint:disable
// @ts-nocheck
// biome-ignore format: autogenerated code
const fileMap = {
  
  "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> Sonnet35 {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> GeminiPro {\n  provider google-ai\n  options {\n    model \"gemini-1.5-pro\"\n    api_key env.GOOGLE_API_KEY\n  }\n}\n\n\n\nclient<llm> Grok {\n  provider \"openai-generic\"\n  options {\n    base_url \"https://api.x.ai/v1/chat/completions\"\n    api_key env.XAI_API_KEY\n    model grok-2-1212\n  }\n}\n\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT4oMini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
  "codenames.baml": "class SpymasterClue {\n  word string @description(\"The one-word clue given by the spymaster\")\n  number int @description(\"Number of words related to the clue\")\n  reasoning string @description(\"Explanation for the chosen clue\")\n}\n\nclass GameState {\n  currentTeam \"red\" | \"blue\"\n  words string[]\n  teamWords string[] @description(\"Words belonging to the current team\")\n  opposingWords string[] @description(\"Words belonging to the opposing team\")\n  assassinWord string\n  revealedCards string[]\n}\n\nclass GameAnalysis {\n  message string @description(\"The AI's analysis of the game state\")\n  suggestedMove string? @description(\"Suggested word to guess if applicable\")\n  moveConfidence \"high\" | \"medium\" | \"low\" @description(\"Confidence in the suggested move\")\n}\n\nfunction GenerateClue(state: GameState) -> SpymasterClue {\n  client \"openai/gpt-4o\"\n  prompt #\"\n    You are the Codenames spymaster for the {{ state.currentTeam }} team.\n\n    Your team's words: {{ state.teamWords }}\n    Opposing team's words (avoid): {{ state.opposingWords }}\n    Assassin word (must avoid): {{ state.assassinWord }}\n    Words already revealed: {{ state.revealedCards }}\n\n    Give a strategic one-word clue to help your team find their words while avoiding others.\n\n    {{ ctx.output_format }}\n\n    {{ _.role(\"user\") }} Based on these words, provide a strategic clue and number.\n  \"#\n}\n\nfunction AnalyzeClue(\n  clue: SpymasterClue,\n  state: GameState\n) -> GameAnalysis {\n  client \"anthropic/claude-3-5-sonnet-latest\"\n  prompt #\"\n    You are a Codenames player on the {{ state.currentTeam }} team.\n    \n    The spymaster has given the clue: \"{{ clue.word }}\" ({{ clue.number }})\n\n    Available words: {{ state.words }}\n    Words already revealed: {{ state.revealedCards }}\n\n    Analyze this clue and suggest whether to guess any words.\n\n    {{ ctx.output_format }}\n\n    {{ _.role(\"user\") }} Analyze this clue and make a recommendation.\n  \"#\n}\n\nfunction ValidateGuess(\n  word: string,\n  clue: SpymasterClue,\n  state: GameState,\n  model: string\n) -> GameAnalysis {\n  client \"openai/gpt-4o\"\n  prompt #\"\n    You are validating a potential guess in Codenames.\n\n    Clue given: \"{{ clue.word }}\" ({{ clue.number }})\n    Proposed guess: \"{{ word }}\"\n    \n    Current game state:\n    - Team: {{ state.currentTeam }}\n    - Available words: {{ state.words }}\n    - Revealed cards: {{ state.revealedCards }}\n\n    {{ ctx.output_format }}\n\n    {{ _.role(\"user\") }} Evaluate whether this word is a good guess based on the clue.\n  \"#\n}",
  "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"typescript\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.77.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode async\n}\n",
  "resume.baml": "// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"openai/gpt-4o\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
}
export const getBamlFiles = () => {
    return fileMap;
}